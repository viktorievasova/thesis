
\chapter{Image Processing Algorithms}
\label{chap:algorithms}

% TODO predchozi kapitolky byly poprehazeny, vynechcany: In the previous chapter we gave an overview of related work and algorithms that were introduced. 
% This chapter is concentrated on some of these approaches in greater detail. 
In this chapter, we introduce image processing algorithms often applied in software solutions with functionality similar to those discussed in Chapter \ref{chap:overview}. % TODO moc zdlouhava veta
Applications reconstructing depth information from photographs or video sequences typically have at their core an algorithm that detects and tracks image primitives (such as corners, blobs, lines, individual pixels, etc.) across several photos/frames. 
This knowledge allows one to estimate the camera parameters, which in turn makes it possible to compute the spacial parameters of the tracked primitives.
% Tohle je asi moc velke rozpatlavani, takze jsem to ani nedopsal: For example, the solution outlined in \todo{reference na Snavelyho} extracts certain image features from individual input photos, matches these features between pairs of photos to obtain tracks, then proceeds to estimate camera parameters and 3D positions of points correponding to these features and finished by visualizing the resulting \pc.

The approaches to computing correspondences between pairs of photos can be divided into \term{sparse correspondence} and \term{dense correspondence} problems. % can be approached from two perspectives. 
In the former, a relatively conservative subset of only the most stable \term{features} are considered and paired. % TODO mozna zvazit, jestli pojem features nezavest nejak formalneji
In the latter, the objective is to obtain a correspondence in the second image for (almost) every pixel of the first image. 
% Thus, dense correspondence algorithms allow us to obtain full diparity map of the first image in the image pair. 
% We can either aim to establish correspondences between a restricted set of primitives, for example just the most stable looking corners detected in the images, or we can aim to establish a correspondence from (almost) every pixel of the reference image. 
% The former approach is called \term{sparse correspondence problem} 

We describe two techniques solving the sparse correspondence problem -- the SIFT \cite{lowe1999} and SURF \cite{surf2006} feature detectors/descriptors -- and then introduce a max-flow algorithm \cite{roy1999} for the dense correspondence problem. %%% optical flow 

% We describe three techniques -- the SIFT feature detector/descriptor, the SURF feature detector/descriptor, and the Graph-cut algorithm. \todo{potrebujeme urcite pridat citace}
% SIFT and SURF are the most used algorithms to detect features.
% Both of them are robust to scale and rotation.
Generally, sparse feature matching algorithms first extract features from each photo. % from a pair of photos of the same scene. % TODO nejsem si jisty, jestli tam napsat "of the same scene", protoze taky mohou byt pouzity k object detection 
If the feature detection is repeatable, most of the features should be detected in all images where the corresponding scene element is visible.
Then, for each feature a \term{descriptor} is generated, typically a high-dimensional vector. 
This descriptor is constructed in a way ensuring invariance to various image transformations. 
For instance, the SIFT descriptor is invariant to translation, rotation, scaling, and image noise. 
Thus, applying any of those transformations to the image should leave the descriptors unaffected, making it possible to match image features using Euclidean metric. 
The SIFT descriptor is also partially invariant to illumination changes and to affine transformations, the latter enabling us to perform matching of images capturing the scene from slightly different viewpoints.
% Achieving full invariance to projective transformations is typically too computationally prohibitive.  
% Usually, a descriptor is affinely invariant, meaning .
% Hence for a feature from different viewpoints should be generated the same vector.
% Since the descriptors are computed, we can match them between a pair of images. 
% Due to the fact that our descriptors are vectors, we can match them according to the distance in the space, using Euclidean distance for example.
 
\section{Scale-Invariant Feature Transform}
\label{sec:sift}
Scale-invariant feature transform (SIFT) was introduced by Lowe in \cite{lowe1999}, building on a previous work by Lindeberg \cite{lindeberg1998}. 
The paper describes both a feature detector and descriptor.
To this day, SIFT remains a popular choice for applications such as panorama stitching and object detection.
It is able to detect objects in images even in the presence of clutter. 
% As already mentioned, the SIFT feature descriptor 

The features are detected in a grayscale version of the input photo and tend to match centers of blob-like image structures.
Each feature is assigned an orientation and a scale, which is used to establish a reference frame for the construction of the descriptor. 
The SIFT feature descriptor is a 128-dimensional vector determined by the gradients around the descriptor.
Since the computation of the descriptor is performed using the abovementioned local reference frame, the resulting descriptor is invariant to transformations that affect this reference frame in a covariant manner but only partially invariant to affine transformations and illumination changes.
We now proceed to describe both the detection and the descriptor construction steps in more detail. 

The feature detection starts by creating the scale-space of the input image, 
% First step to reach the set of keypoints is creating a scale-space of the input image.
which is a series of images constructed by blurring the original one using Gaussian filters of increasing variance. 
(The process is performed on a downsampled image for larger kernel sizes to increase computational efficiency.)
% The original image is blurred several times with larger and larger Gaussian kernel, then it is downsized and this process is applied again to the subsampled image. 
After constructing the scale-space, we subtract neighbouring images of the scale-space to obtain a series of differences of Gaussians (DoG) images. 
A DoG image is thus equal to: 
\[ D_{\sigma_1, \sigma_2}(x, y) = G_{\sigma_1} \convolution I(x, y) - G_{\sigma_2} \convolution I(x, y), \]
where $I(x, y)$ is the original image, $\convolution$ presents convolution, $G_{\sigma_i}$ is a Gaussian kernel, and $\sigma_i$ is the its deviation. 
% TODO mozna detailneji: In our case, $\sigma_1$ and $\sigma_2$ are the kernel sizes of the neighbouring ...
Hence
\[
D_{\sigma_1, \sigma_2}(x, y) = 
\left[
\left( \frac{1}{\sqrt{2\pi \sigma _1^{2}}} \cdot e^{-\frac{x^{2}+y^{2}}{2\sigma_1^{2}} } \right) -
\left( \frac{1}{\sqrt{2\pi \sigma _2^{2}}} \cdot e^{-\frac{x^{2}+y^{2}}{2\sigma_2^{2}} } \right )
 \right ] * I(x, y).
\]
\todo{nasledujici vete bohuzel vubec nerozumim} These convolved images are grouped by octave, where an octave corresponds to the doubling of $\mathb{\sigma}$.

\todo{nejak v tom textu budeme muset zjednotit feature/keypoint} We can now define candidate keypoints as local maxima and minima of $D_{\sigma_1,\sigma_2}(x, y)$.
This is done by comparing each pixel with its neighbouring pixels and also with the corresponding pixels in neighbouring DoG images. 
% TODO tohle je mozna uz prilis rozebiraci: If the pixel value is the largest or the lowest among all compared pixels, it is selected as a candidate feature point.
These are exactly the areas we are looking for -- spots of high contrast. % as illustrated on Figure ... 
% TODO example to show black and white area in DoG image + explanation

Usually, the detection of scale-space extrema results is a large number of candidate keypoints.
However, some of them are local extrema created by image noise or the responses of the operator along edges. 
Such structures are unstable and should be discarded from the set of candidate keypoints.
This is performed by fitting a quadratic function to the DoG data and examining its shape. 
If the function is too shallow or narrow, the candidate keypoint is discarded. 
For each candidate we perform an interpolation of nearby image data.
This is done using quadratic Taylor expansion of the DoG function around the detected point:
\[
D(\mathbf{x}) = D + \frac{\partial D^{T}}{\partial \mathbf{x}}\mathbf{x} + \frac{1}{2}\mathbf{x^{T}}\frac{\partial ^{2}D}{\partial \mathbf{x^{2}}}\mathbf{x}
\]
where $\mathbf{x} = (x, y)$ is the offset from the point.
If the offset value is less than $0.03$, it indicates low contrast and a high probability of being affected by noise and the candidate is discarded.
Otherwise we keep the keypoint.

The next step is to analyse the area around individual keypoints to generate the descriptor. 
The local scale of the keypoint is determined by the level of scale-space in which the keypoint was detected. \todo{urcite je to jeste podle neceho dalsiho}
To achieve invariance to rotation, local orientation must be determined as well. 
First, we compute a histogram of orientations of image gradients around the keypoints location.
The peaks of the histogram represent dominant orientations.
The orientations corresponding to the highest peak are assigned to the feature. 
If there are several peaks of magnitude within 80\% of the highest peak, mutiple keypoints are generated -- one for each such orientation.

The descriptor is then constructed by considering several histograms in the four quadrants of the reference frame of the keypoint. 
Typically, each quadrant is divided into four subregions. 
In each of the resulting sixteen subregions, we construct a histogram of orientations of gradients sampled regularly in the subregion. 
Concatenating the values in these histograms results in a 128-dimensional vector.

\section{Speeded-Up Robust Features}

Another popular feature detector/descriptor is the Speeded Up Robust Features (SURF) algorithm, introduced by Bay et al. \cite{surf2006}. 
It is influenced by the SIFT algorithm described above and is based on the computation of Haar wavelet responses and Hessian-matrix approximation. 
Similarly to SIFT, keypoints are again detected in a scale-space to achieve invariance to scaling. 
% To build a robust descriptor we need to detect keypoints that are invariant to scale thus we create a scale-space.
% That is because often is required to detect objects in different distances.
% A scale-space is usually implemented as a pyramid of subsequently smoothed images by Gaussian filter as described in the previous section.
In this approach, however, the scale-space is not constructed explicitly and integral images are employed instead to descrease the computation time. % , see previous chapter to get detailed information about integral images.
% So we do not have to iteratively apply the same filter, but we can reach the scaling by fast computing of integral image where the speed is independent to the size of filter.
%%more about scaling...

The detection of interest points is performed using an approximation of the Hessian-matrix.
The Hessian for a point $\x = (x, y)$ in an image $I$ at scale $\sigma$ is defined as
$$H(\x, \sigma) = \begin{pmatrix} 
L_{xx}(\x, \sigma) & L_{xy}(\x, \sigma) \\ 
L_{yx}(\x, \sigma) & L_{yy}(\x, \sigma)
\end{pmatrix},$$
where $L_{xx}$ presents the convolution of the Gaussian second order derivative with the image:
\[
L_{xx}(\x, \sigma) = \frac{\partial ^{2}}{\partial x^{2}}g(\sigma) * I
\]
and similarly for $L_{xy}$ and $L_{yy}$
\[
L_{xy}(\x, \sigma) = \Big(\frac{\partial ^{2}}{\partial x \partial y}g(x, y)\Big)(\sigma) * I, 
L_{xx}(\x, \sigma) = \Big(\frac{\partial ^{2}}{\partial y^{2}}g(x, y)\Big)(\sigma) * I.
\]
\todo{radsi bych se tomu vektoru x vyhnul a napsal x, y -- kvuli tem parcialnim derivacim; hlavne dejme pozor, abychom pouzivali jednotne znaceni tech derivaci -- stejne jako v podkapitole o tom, co je Gaussovska derivace}

% Since the Gaussian is considered as over-rated, in this approach we use box-filters instead and obtain speeded-up variation.
A box filter approximation of a convolution with a Gaussian-derivative kernel can be computed quickly using integral images. 
\todo{mozna nekde rict, jak presne se to aproximuje, nejlip obrazkem; asi by to mohlo byt v sekci o integralnim obrazu}
% The size of a box filter corresponds to the size of the kernel. 
The approximations are computed for every scale of the scale-space and 
the points that are simultaneously local extrema of both the determinant and the trace of the Hessian matrix are chosen as candidate keypoints. 
The trace of the Hessian matrix corresponds to the Laplacian of Gaussians (LoG) \todo{meli bychom rict LoG ceho}.
If we denote the results of applying the above-mentioned box filters as $D_{xx}$, $D_{yy}$ and $D_{xy}$, we get: 
\[
\det(H) = D_{xx} D_{yy} - \omega^{2} D_{xy}^{2}.
\]
\todo{porad neni uplne dobre definovno, co je to ten box filter; idealni by bylo pridat to do sekce o integralnim obrazu}

Similarly to SIFT, the SURF descriptor is a vector describing the distribution of intensity values within the neighbourhood of the keypoint. 
The difference lies mainly in the fact that the SURF descriptor is constructed using Haar wavelet responses around the point of interest. % TODO skutecne sum? mozna mi neni jasne, v jakem smyslu
At first, to be invariant to image rotation, a dominant orientation is established for each detected keypoint. 
The Haar wavelet responses in $x$ and $y$ direction are calculated 
and they are summed within a sliding orientation window of size $\mathb{\frac{\pi}{3}}$ afterwards. \todo{tady by mi nebylo jasne, co se tam s tim sliding window dela; v tuhle chvili mam v ruce dve cisla, hodnotu derivace podle x a podle y -- jak do toho vstupuje sliding window?}
The summed responses yield a local orientation vector. \todo{jenom jeden vektor? Jak to, ze zahy budu vybirat ten nejdelsi z nich?}
The longest such vector is used to establish the orientation of the feature. 

% The first step to extract the descriptor is constructing a square region around the keypoint aligned with dominant orientation.
The local orientation and scale is again used to obtain a reference frame ensuring that the resulting descriptor is not affected by rotation and scaling of the input image. 
The size of the inspected region around the keypoint is set to $20s$, where $s$ is the scale where it was detected. \todo{nemelo by $s$ byt nejak svazane s rozptylem $\sigma$?}
We split the region into $4 \times 4$ regular square subregions and for each of them we compute Haar wavelet responses $d_x$ and $d_y$
in a $5 \times 5$ grid. \todo{tady porad rikame ``pocitame Haar wavelet responses'' ale jde nam o to vypocitat ty derivace; ja bych to napsal tak, ze se na zacatku nadefinuji ty derivace aproximovane temi integral image a pak uz se jen pouziva dokola odkazuje se na to nejakym konkretnejsim terminem; navic mi neni jasne, jaky je rozdil mezi temi box filtery a Haar wavelety} 
The sum of the responses for $d_x$ and $d_y$ and their absolute values $|d_x|$, $|d_y|$ separately results in a descriptor $v = (\sum{d_x}, \sum{d_y}, \sum{|d_x|}, \sum{|d_y|})$ for each subregion.
Concatenating this for all $4 \times 4$ subregions gives $64$-dimensional vector descriptor.

Matching the SURF descriptors between two images can be speeded-up by including the trace of the Hessian (Laplacian) to the process of finding corresponding keypoints.
We exploit the sign of Laplacian that distinguishes bright blobs on dark background from the reverse situation.
In the matching stage we compare only features if they have the same sign -- type of contrast. 
This costs no additional computation time since this quantity has already been computed at a previous stage.

\section{SIFT vs. SURF}
Both SIFT and SURF are popular feature descriptors based on the distribution of intensity around the interest point.
% TODO: kdyz uz bysme nasledujici chteli tvrdit, je nutne odcitovat nejaky benchmarkovaci paper: SIFT used to be evaluated as the most robust and distinctive descriptor for feature matching.
SURF has been shown to have similar performance to SIFT, but being much faster. \todo{odkaz na nejakou studii porovnavajici ruzne detektory/deskriptory}
The work \cite{surf2006} highlights the fast computation of the descriptor.
SURF describes image three times faster than SIFT. \todo{jestli je tohle napsane v tom puvodnim clanku, tak ty predchozi vety spojme a napisme to tak, at ta informace z toho je patrna}
The element enabling this speed-up are the approximations using integral images. 
Also, the SURF descriptor is a 64-dimensional vector, enabling faster computation of the Euclidean distance between the descriptors compared to the 128-dimensional SIFT descriptors. % in the comparison to 128 integers describing SIFT feature.
% Hence the SIFT matching costs more calculation time.
Furthermore, due to the global integration of the SURF-descriptor, it stays more robust to various perturbations than the SIFT descriptor. \todo{potrebujeme napsat presneji, zejmena by se hodil odkaz na to, odkad to tvrzeni je cerpano}

SURF offers invariance to rotation and limited levels of image blurring.
On the other hand, SIFT is more robust under illumination changes and viewpoint changes. \todo{znovu neni jasne, jestli popisujeme vlastni zkusenost nebo neco z nejakeho clanku (jakeho?)}

\section{Max-flow dense correspondence algorithm}

% In computer vision there is a general task of stereo correspondence.
% It is a problem of discovering the closest match between points of two pictures typically taken from different positions.
% Presumably, rectification of the images considerably simplifies the situation. 
% Sometimes this constraint can be done.
% Once the correspondences are solved, we obtain a disparity map and it can be exploited to reconstruct the positions and distances of the cameras.

% TODO: Nasledujici patri do uvodu kapitoly a taky jsem to tam presunul. 
% Generally, there are two approaches to find the corresponding points.
% One of them is to detect features and interest points in an image and then find corresponding points in the other one.
% Only these high distinctiveness points are matched, hence it is called sparse stereo matching.
% The second way, dense stereo matching, is to match as many pixels as possible.

% In 1999, Roy published an algorithm %\cite{roy1998} 
% \cite{roy1999} to solve the stereo correspondence problem formulated as a maximum-flow problem in a graph.
% It solves efficiently maximum-flow and gives a coherent minimum-cut that presents a disparity surface for the whole image.
% This way provides more accurate depth map then the classic algorithms and the result is computed in shorter time in addition.

%The matching space is defined as a projective 3-d volume (to allow pyramids as well as cubes) formed

\section{Lucas-Kanade algorithm for optical flow} 


