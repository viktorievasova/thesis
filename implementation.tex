\chapter{Implementation}
\label{chap:implementation}

In this chapter, we describe how we connected the computer vision algorithms described in Chapter \ref{chap:algorithms} to obtain a program solving the task of 3D reconstruction on a mobile phone. 
The result of this work is an Android application that allows the user to take photos with a camera, browse them and pick a pair of images to reconstruct. 
The reconstructed image is then visualized in 3D using OpenGL ES.
The properties of the input pair of photos are described in Section \ref{prob}.

The following are the main obstacles we are faced with when trying to solve this problem: 
\begin{itemize}
\item mobile photos typically have a limited computational power, 
\item they have limited operational memory, 
\item most importantly, the optical properties of digital cameras included in a typical mobile phone are of mediocre quality at best. 
\end{itemize} 
The last property is particularly troublesome for us, since it implies -- among other problems -- that the captured image is defomed by an unknown non-linear transformation. 
% This is an issue often encountered in professional-grade cameras as well. 
% However, in these cases, this distortion can be efficiently estimated and accounted for since it typically behaves in a constrained manner. 
% For cheap mobile phone cameras, this is not possible and thus we have to keep in mind that 
This implies that the model of projective camera of Section \ref{sec:projective} and the epipolar constrains of Section \ref{sec:epi} are only a rough approximation. 
However, this significantly restricts the choice computer vision algorithms to apply. 
For example, algorithms for the dense reconstruction problem are often heavily dependent on having a precise model of epipolar constrains in the image pair. 

We solve this problem using a combination of sparse feature matching and the classical optical flow algorithm, both described in Chapter \ref{chap:algorithms}. 
Specifically, we employ the SURF algorithm because it has better computational efficiency compared with the SIFT algorithm.
Sparse feature matching can be successfully performed without any kind of epipolar constraint and thus, the innacuracy of the camera model is not an issue. 
However, the resulting 3D information is only a sparse 3D point cloud reconstruction of the scene. 
Since this is insufficient for our purposes we then repeat optical flow calculation on pairs of neighbourhoods of corresponding features. 

In XXX we discuss this method in detail. 
The final section is devoted to technical aspects of the implementation. 

\section{Implementation outline}
\label{sec:impl_outline}
In the this section we describe the way how was the task solved, what algorithms were chosen and how was the application implemented.
%To implement the Android application, several subtask must be completed.
%At first it is the graphical user interface and handling the camera which is managed through Android Activities and xml files as we described in chapter \ref{chap:android}.
%Secondly, we deal with the calculation, which consist of the image registration, the keypoint detection, the keypoint matching and solving the dense correspondence problem.

To solve our task, we followed these steps:
\begin{itemize}
\item at first we find the initial relative position of the pair of input images,
\item then we detect and match SURF keypoints from which is chosen the most robust match to specify more accurate relative position,
\item then we detect larger amount of SURF keypoints. The matching process uses the information of the relative position estimated in the previous step,
\item at last, we get the dense correspondences by detecting even more matches using the optical flow algorithm.
\end{itemize}

Finding the initial relative position of the pair of the images is implemented by using Sum of absolute differences (described in Section \ref{sec:metrics}).
At first, we create a scale-space, find the overlap of the images in the lowest scale and by upscaling specify the overlapping area more accurately.
In this way the registration runs in approximately two seconds for a pair of the expected input.

%Image registration (finding the relative position of the pair of the images) is implemented by using Sum of absolute differences (described in \ref{sec:metrics}).
%At first, we create a scale-space from the input pair of images, so we can find relatively fast the overlap with minimal difference of intensities of the image pair by comparing each possible overlap in the lowest scale. 
%When we have the approximate overlap, we take the image of the scale above and try to find better result by shifting the matched area in the 5-pixels range.
%This we repeat until we look over all of the levels of the scale-space or for computational efficiency we estimate the result satisfiable when the width or hight of the investigating scale image is higher then a constant value.
%Because we expect the image input in the size of approximately 1000 $\times$ 800 pixels, for the experiments in our work we set the constant to 200. 

\begin{figure}[h]
\centerline{
\includegraphics[width=4.5cm]{img/ema_overlap.png}
\includegraphics[width=4.5cm]{img/ema_buckets.png}}
\caption{Left: The result of the registration where the process of upscaling was stopped when the height of the image was over 200px. Right: The division of an image into square-boxes.}
\label{fig:overlap_and_buckets}
\end{figure}

The next step of the process is detection of keypoints which are detected with the SURF detector.
We divide the images into square-boxes of the same size and to each square-box we assign an array of keypoints situated in it.
This saves the computational time when finding the match -- instead of comparing every possible correspondence between the images we compare only those which are situated in the square-boxes located in the corresponding areas.
At first we find the most robust match from only the SURF keypoints where the response is higher than 4000.
Based on this match we estimate the direction of the shift which gives us more accurate relative position of the input pair of images.

%The next step of the process is detection of keypoints which are detected with the SURF detector.
%We divide the images into square-boxes of the same size and to each square-box we assign an array of keypoints situated in it.
%Because of the previous calculation of approximate overlap we do not have to match all keypoints in the image, but we choose only the keypoints lying in the overlap.
%To identify the relative position of the image pair better we detect the most robust keypoint match from our SURF keypoint set and calculate the vector defining the direction of the shift of the keypoint.
%To find this robust match we choose only the keypoints with response higher than 4000 and each of them we try to match with one keypoint extracted in the second image but only in the corresponding 30 $\times$ 30 pixels area determined due the previous image registration.
%To decrease the computational time we use the pre-calculated square-boxes to find the keypoints of the second image lying in this area.
%To avoid mismatches we reject the matches where the matched keypoints differs too much in the orientation or if there is more than one obvious potential points for the match.

\begin{figure}[h]
\centerline{
\includegraphics[width=9cm]{img/ema_direction.png}}
\caption{The most robust match chosen from the keypoints with response higher than 4000. According to this match the more accurate relative position of images is estimated.}
\label{fig:robust_match}
\end{figure}

In the next step we match the detected SURF keypoints one more time.
For each keypoint we calculate the corresponding area of the surroundings in the shape of a rectangle oriented in the direction of the shift. 
To avoid mismatches we reject the matches where the matched keypoints differs too much in the orientation or if there is more than one obvious potential points for the match.

%When we know the more accurate relative position of the images, we investigate the keypoints one more time.
%Now we iterate all of them. 
%Each keypoint in the first image we try to match with a keypoint in the estimated corresponding surrounding area in the second image in the shape of oriented rectangle computed based on the more accurate relative image position.
%This oriented rectangle actually consists of two rectangles -- one large and one smaller localised in the middle of the previous one.
%The corresponding keypoint is accepted only if its located in the inner one.
%The size of the larger rectangle is set to 60 $\times$ 120 pixels.
%The inner rectangle after some experiments was set to the 10\% of width and 20\% of height.
%It was shown that it gives better results than only a bit larger window of the the width of 20\% of width and 35\% of height size.
%Matched keypoints which differs too much in the orientation or are not obviously the best match we reject again.
%We can see the comparison in Figure \ref{fig:matching_comparison}.


%Dense correspondence: optical flow::
At this point we have relatively robust matches for sparse correspondence. 
For the calculation of the depth information we need to extract more corresponding points.
Assuming the difference of images is limited, we use for this purpose optical flow algorithm. 
For each SURF keypoint in the first image we detect corners or other features acceptable for the tracking algorithm in its 70 $\times$ 70 pixels area.

\begin{figure}[h]
\centerline{
\includegraphics[width=4.0cm]{img/trackingpoints1.png}
\includegraphics[width=4.0cm]{img/trackingpoints2.png}}
\centerline{
\includegraphics[width=4.0cm]{img/trackingpoints1_filtered.png}
\includegraphics[width=4.0cm]{img/trackingpoints2_filtered.png}}
\caption{70 $\times$ 70 windows of the pair of the input images with optical flow matches.
 In the first line all the detected correspondences are shown, in the second line are only those which were accepted after filtering.}
\label{fig:trackingpoints}
\end{figure}

Calculating the optical flow we get the corresponding points in the second image.
With high probability some of the results will be influenced by noise.
To avoid these mismatches we calculate the variation of the distances between each match.
If the variation is higher than 300 we discard all of the detected optical flow matches.

The result is visualized in OpenGL ES.
Every keypoint is represented as a triangle in a space with the depth calculated from the correspondence.
At first, the user can view the rough model created from the results of SURF matching.
Each time some of the dense corresponding points are calculated with optical flow algorithm, the model is updated.

\begin{figure}[h]
\centerline{
\includegraphics[width=4.0cm]{img/rectangle_w_02_h_035_croped.png}
\includegraphics[width=12.1cm]{img/matching_w_02_h_035.png}}
\centerline{
\includegraphics[width=4.0cm]{img/rectangle_w_01__h_02_croped.png}
\includegraphics[width=12.1cm]{img/matching_w_01_h_02.png}}
\caption{Comparison of the result of matching with different sizes of the oriented rectangles. 
At the top left is a sample of the oriented rectangle where the width of inner rectangle is 20\% and height is 35\% of the size of the large one. 
Next to it is the result of matching the SURF keypoints where for each keypoint in the first image is estimated this rectangle for the detection of the corresponding point in the second image.
Below is the result of same matching with smaller rectangular area -- the width of the inner rectangle is reduced to 10\% and hight to 20\% of the size of the large one. 
We can see that in the second case there is less mismatches than in the first one.}
\label{fig:matching_comparison}
\end{figure}

