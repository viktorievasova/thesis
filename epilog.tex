\chapter*{Conclusion and future work}
\addcontentsline{toc}{chapter}{Conclusion and future work}

We have described and implemented a mobile application for 3D reconstruction from images. 
The application has been tested on a Sony ST27i mobile device with a 5 megapixel camera and 1 Ghz Cortex-A9 processor, on which it is able to reconstruct a scene in a manner of several seconds. 
Moreover, thanks to the asynchronous implementation of the reconstruction and visualization threads, the application continues to refine the reconstruction while the user is viewing the result.
Another advantage of our approach is the relative independence on camera quality. 
In particular, the application does not depend on any algorithm that would require a precise camera calibration data. 
Although it would theoretically be possible to obtain a part of the internal calibration of the phone's camera, it would likely require the user to calibrate the camera using, for example, a chessboard calibration pattern -- something not all users would have the patience for. 
Therefore, this property is highly desirable. 

The application has been tested on input samples from Figure \ref{fig:input_samples}, where we have obtained satisfiable results as discussed in Chapter \ref{chap:eval}.
However, our implementation fails and does not give good results on noisy images or on images with not enough texture. 
In such kinds of scenes, the features are matched incorrectly or not even detected.
Another issue is the behavior when the photos of the scene differ in illumination, since the algorithm registering the two images using the sum of absolute differences does not correct for this.
In these cases, it is likely that the photos will be registered incorrectly which affects all the other algorithms in our reconstruction pipeline. 
This could be fixed by using an appropriate metric, for example mutual information, which is invariant to global  color transformations. 

Our implementation could be also extended to be able to process $n$ photos of a single scene at once. 
It would be necessary to find the relative position of all pairs of images.
This does not have to be calculated explicitly, but we could estimate the position of two images when having the information about their relative position with respect to a third image.
Detection and matching of the SURF features would be applied only to the pairs with an overlapping part.
Most of the implemented methods can be easily extended for this purpose.

